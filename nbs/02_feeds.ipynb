{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeds\n",
    "\n",
    "> Series of utility tools to parse and manage Small Web feeds (RSS and Atom feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp feeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "import feedparser\n",
    "import os\n",
    "import re \n",
    "import requests\n",
    "import sqlite3\n",
    "from collections import namedtuple\n",
    "from langdetect import detect\n",
    "from rich import print\n",
    "from rich.progress import Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeds DB\n",
    "\n",
    "We will want to save diffent kind of information related to the feeds we process. We will save that information locally in a lightweigth SQLite database. Here are different kind of things we will want to save:\n",
    "\n",
    " - feed's ID (private key)\n",
    " - its language\n",
    " - number of entries\n",
    " - last time we downloaded it\n",
    " - type of feed\n",
    " - feed's URL\n",
    " - feed's title\n",
    " - feed's description\n",
    " - feed's author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def connect_feeds_db() -> sqlite3.Connection:\n",
    "    \"\"\"Connect to the feeds database\"\"\"\n",
    "    db_folder = os.environ.get('DB_PATH').rstrip('/')\n",
    "\n",
    "    # create the db folder if not already existing\n",
    "    if not os.path.exists(db_folder):\n",
    "        os.makedirs(db_folder)\n",
    "\n",
    "    conn = sqlite3.connect(f\"{db_folder}/feeds.db\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_feeds_db(conn: sqlite3.Connection):\n",
    "    \"\"\"Create the feeds database\"\"\"\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS feeds\n",
    "                 (id str PRIMARY KEY, \n",
    "                  url text,                    \n",
    "                  title text, \n",
    "                  description text, \n",
    "                  lang str,\n",
    "                  feed_type str,\n",
    "                  license str)''')\n",
    "    c.close()\n",
    "    conn.commit()\n",
    "\n",
    "def create_articles_db(conn: sqlite3.Connection):\n",
    "    \"\"\"Create the articles database\"\"\"\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "                 (id str PRIMARY KEY, \n",
    "                  feed_id str, \n",
    "                  title text, \n",
    "                  content text, \n",
    "                  creation_date datetime,\n",
    "                  lang str,\n",
    "                  license str, \n",
    "                  FOREIGN KEY (feed_id) REFERENCES feeds(feed_id))''')\n",
    "    c.close()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync Feeds\n",
    "\n",
    "The local feeds needs to be synchroninzed with the Small Web index. Most of them will be new, but it is possible that some of the previous feeds gets removed from the feeds index. In that case, we have to remove the feed from the local system and the SQL database. The process is as fellow:\n",
    "\n",
    " 1. check if some of the feeds got removed from the index\n",
    "    1. if so, remove the feed from the local system\n",
    "    2. remove the feed from the SQL database\n",
    " 2. if the feed is not already on the file system, create a unique folder name for each of the new feed\n",
    " 3. create a `DDMMYYYY` folder under the ID of the feed in the `FEEDS_PATH` folder\n",
    " 4. download the feed's file in that folder\n",
    "\n",
    "The local folder and file structure should be:\n",
    "\n",
    " - `FEEDS_PATH`\n",
    "   - `feed_unique_folder`\n",
    "     - `DDMMYYYY`\n",
    "       - `feed.xml`\n",
    "     - `DDMMYYYY`\n",
    "       - `feed.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Feeds\n",
    "\n",
    "The first step is to get le list of feeds for the Small Web. That list is available from the [Kagi Small Web index](https://github.com/kagisearch/smallweb/blob/main/smallweb.txt). Then for each of those feed in the list, we will download and save them locally in the `FEEDS_PATH` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_small_web_feeds() -> list:\n",
    "    \"\"\"Get smallweb feeds from KagiSearch's github repository\"\"\"\n",
    "    response = requests.get('https://raw.githubusercontent.com/kagisearch/smallweb/main/smallweb.txt')\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # split the response into a list of lines\n",
    "        return response.text.splitlines()\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(get_small_web_feeds()) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed ID\n",
    "\n",
    "We build the unique ID of a feed from its URL. We use the following steps:\n",
    "\n",
    " 1. For every character, if it not an alpha numeric character, we replace it with a `-`\n",
    "\n",
    "This method is used to make sure we can use the ID to create files and directories on the local file system, as a private key in the BD, while keeping the ID readable. It could duplicate IDs if a non-alpha numeric character is the only differenciator of a URL, in which case both will be replaced by a `-` and the IDs will clash. But this is unlikely in short term and is good enough for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_feed_id_from_url(url: str) -> str:\n",
    "    \"\"\"Get the feed id from a feed url\"\"\"\n",
    "    # Make feed folder name from URL by keeping alphanumeric characters only, and replacing everything else with a dash\n",
    "    return ''.join(ch if ch.isalnum() else '-' for ch in url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_feed_id_from_url('https://example.com/feed.xml') == 'https---example-com-feed-xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Removed Feeds From Index\n",
    "\n",
    "It is possible that previously downloaded feeds get removed from the Small Web index. In this case, we get the latest version of the Small Web index, detect which was was removed, and remove it from the file system and the SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def gen_ids_index(index: list) -> list:\n",
    "    \"\"\"Return a list of IDs of the feeds in the index\"\"\"\n",
    "    return [get_feed_id_from_url(url) for url in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['https://example.com/feed.xml']\n",
    "index2 = gen_ids_index(index)\n",
    "assert index2 == ['https---example-com-feed-xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def process_removed_feed_from_index(index: list):\n",
    "    \"\"\"Process all the feeds that got removed from the SmallWeb index\"\"\"\n",
    "\n",
    "    conn = connect_feeds_db()\n",
    "    c = conn.cursor()\n",
    "    ids_index = gen_ids_index(index)\n",
    "\n",
    "    # get all the current feeds from FEEDS_PATH\n",
    "    for folder in os.listdir(os.environ.get('FEEDS_PATH')):\n",
    "        if folder not in ids_index:\n",
    "            # remove the feed folder\n",
    "            os.system(f'rm -rf {folder}')\n",
    "\n",
    "            # remove from the database            \n",
    "            \n",
    "            c.execute(f\"DELETE FROM articles WHERE feed_id = '{folder}'\")\n",
    "            c.execute(f\"DELETE FROM feeds WHERE id = '{folder}'\")\n",
    "            conn.commit()\n",
    "\n",
    "    c.close()\n",
    "    conn.close()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def download_feed(url: str):\n",
    "    \"\"\"Download a feed from a given url\"\"\"\n",
    "\n",
    "    # create a folder for the feed if not already existing\n",
    "    folder_path = f\"{os.environ.get('FEEDS_PATH').rstrip('/')}/{get_feed_id_from_url(url)}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "    # Create the DDMMYYYY folder if it is not already existing\n",
    "    date_folder_path = f\"{folder_path}/{datetime.datetime.now().strftime('%d%m%Y')}\"\n",
    "    if not os.path.exists(date_folder_path):\n",
    "        os.makedirs(date_folder_path)\n",
    "\n",
    "    # only download if feed.xml is not existing\n",
    "    if not os.path.exists(f\"{date_folder_path}/feed.xml\"):\n",
    "        # Download the feed\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Save the feed to the DDMMYYYY folder\n",
    "            with open(f\"{date_folder_path}/feed.xml\", 'w') as f:\n",
    "                f.write(response.text)\n",
    "            #print(f\"Downloaded feed from {url} to {date_folder_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download feed from {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync all the feeds from the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def sync_feeds():\n",
    "    \"\"\"Sync all feeds from smallweb\"\"\"\n",
    "\n",
    "    feeds = get_small_web_feeds()\n",
    "\n",
    "    print(\"[cyan] Clean removed feed from the Small Web index...\")\n",
    "    process_removed_feed_from_index(feeds)\n",
    "\n",
    "    with Progress() as progress:\n",
    "        task = progress.add_task(\"[cyan]Downloading feeds locally...\", total=len(feeds))\n",
    "\n",
    "        def progress_indicator(future):\n",
    "            \"Local progress indicator callback for the concurrent.futures module.\"\n",
    "            if not progress.finished:\n",
    "                progress.update(task, advance=1)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for url in feeds:\n",
    "\n",
    "                futures = [executor.submit(download_feed, url)]\n",
    "\n",
    "                # register the progress indicator callback for each of the future\n",
    "                for future in futures:\n",
    "                    future.add_done_callback(progress_indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "\n",
    "We use the library [langdetect](https://pypi.org/project/langdetect/) to detect the language of a feed. We use the `detect` method of the library. We tried other avenues like Hugging Face madels, but the language detection performance and the processing performaces with not justifying the additional complexity for now (results were worse and much slower). You can check the file `01_language_detection.ipynb` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def detect_language(text: str):\n",
    "    \"\"\"Detect the language of a given text\"\"\"\n",
    "\n",
    "    # remove all HTML tags from text\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "\n",
    "    # remove all HTML entities from text\n",
    "    text = re.sub('&[^;]+;', '', text)\n",
    "\n",
    "    # remove all extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # return if the text is too short\n",
    "    if len(text) < 64:\n",
    "        return ''\n",
    "\n",
    "    # limit the text to 4096 characters to speed up the \n",
    "    # language detection processing\n",
    "    text = text[:4096]\n",
    "\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        # if langdetect returns an errors because it can't read the charset, \n",
    "        # simply return an empty string to indicate that we can't detect\n",
    "        # the language\n",
    "        return ''\n",
    "\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_language('This is a test') == ''\n",
    "assert detect_language('This is a test' * 128) == 'en'\n",
    "\n",
    "assert detect_language('Ceci est un test') == ''\n",
    "assert detect_language('Ceci est un test' * 128) == 'fr'\n",
    "\n",
    "assert detect_language('これはテストです') == ''\n",
    "assert detect_language('これはテストです' * 128) == 'ja'\n",
    "\n",
    "assert detect_language('이것은 테스트입니다') == ''\n",
    "assert detect_language('이것은 테스트입니다' * 128) == 'ko'\n",
    "\n",
    "assert detect_language('<br /><br /><br /><br /><br /><br /><br /><br /><br />This is a test') == ''\n",
    "assert detect_language('<br /><br /><br /><br /><br /><br /><br /><br /><br />This is a test' * 128) == 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a Local Feed\n",
    "\n",
    "For any given feed URL, let's parse the local feed we downloaded for it and return an internal dictionary that represents it, whatever if it is a RSS or Atom feed. The internal representation of an small web article if represented by a `namedtuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "Feed = namedtuple('Feed', ['id', 'url', 'title', 'description', 'lang', 'feed_type', 'license'])\n",
    "Article = namedtuple('Article', ['url', 'feed', 'title', 'content', 'creation_date', 'lang', 'license'])\n",
    "\n",
    "def parse_feed(feed_path: str, url: str):\n",
    "    \"\"\"Parse a feed from a given path and url\"\"\"\n",
    "\n",
    "    feed_id = get_feed_id_from_url(url)\n",
    "    parsed = feedparser.parse(feed_path)\n",
    "\n",
    "    feed_title = parsed.feed.get('title', '')\n",
    "    feed_description = parsed.feed.get('description', '')\n",
    "\n",
    "    feed = Feed(feed_id,\n",
    "                url,\n",
    "                feed_title, \n",
    "                feed_description,\n",
    "                detect_language(feed_title + feed_description),\n",
    "                parsed.get('version', ''),\n",
    "                parsed.get('license', ''))\n",
    "\n",
    "    articles = []\n",
    "    for entry in parsed.entries:\n",
    "        article_title = entry.get('title', '')\n",
    "        article_content = entry.description if 'description' in entry else entry.content if 'content' in entry else ''\n",
    "        articles.append(Article(entry.get('link', ''),\n",
    "                                feed_id,\n",
    "                                article_title,\n",
    "                                article_content,\n",
    "                                entry.published if 'published_parsed' in entry else datetime.datetime.now(),\n",
    "                                detect_language(article_title + article_content),\n",
    "                                entry.get('license', '')))\n",
    "    return feed, articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync Feeds DB from Local Cache\n",
    "\n",
    "We do download all and every feeds locally and save them in a time stamped folder of the day where they were downloaded. We proceed that way such that we don't have to redownload all the feeds every time we change an internal process that requires us to parse the feeds again. We can just parse the local cache of the feeds we downloaded.\n",
    "\n",
    "The synchronization occurs by simply creating one transaction per feed using INSERT OR INGORE which appears to be the fastest way to only add the new feeds and ignore the one that are already in the DB. This is also by far the simplest logic to implement and to reason about.\n",
    "\n",
    "If the database is empty, then it will be fully populated with the cache of the provided DDMMYYY as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def sync_feeds():\n",
    "    \"\"\"Sync all feeds from smallweb\"\"\"\n",
    "\n",
    "    feeds = get_small_web_feeds()\n",
    "\n",
    "    print(\"[cyan] Clean removed feed from the Small Web index...\")\n",
    "    process_removed_feed_from_index(feeds)\n",
    "\n",
    "    with Progress() as progress:\n",
    "        task = progress.add_task(\"[cyan]Downloading feeds locally...\", total=len(feeds))\n",
    "\n",
    "        def progress_indicator(future):\n",
    "            \"Local progress indicator callback for the concurrent.futures module.\"\n",
    "            if not progress.finished:\n",
    "                progress.update(task, advance=1)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for url in feeds:\n",
    "\n",
    "                futures = [executor.submit(download_feed, url)]\n",
    "\n",
    "                # register the progress indicator callback for each of the future\n",
    "                for future in futures:\n",
    "                    future.add_done_callback(progress_indicator)\n",
    "\n",
    "def sync_feeds_db_from_cache(ddmmyyyy: str = datetime.datetime.now().strftime('%d%m%Y')):\n",
    "    \"\"\"Sync the feeds database from the cache. The cache by default to use is the one from today.\n",
    "    It is possible to use a different cache by passing a different date in the format DDMMYYYY\"\"\"\n",
    "    conn = connect_feeds_db()\n",
    "\n",
    "    c = conn.cursor()\n",
    "\n",
    "    urls = get_small_web_feeds()\n",
    "\n",
    "    with Progress() as progress:\n",
    "\n",
    "        task = progress.add_task(\"[cyan]Synching feeds DB from local cache...\", total=len(urls))\n",
    "\n",
    "        for url in urls:\n",
    "            feed_id = get_feed_id_from_url(url)\n",
    "            feed_folder = f\"{os.environ.get('FEEDS_PATH').rstrip('/')}/{feed_id}\"\n",
    "\n",
    "            # it is possible the feed was not reachable last time it got scraped\n",
    "            if not os.path.exists(feed_folder):\n",
    "                progress.update(task, advance=1)\n",
    "                continue\n",
    "\n",
    "            # get the feed.xml path\n",
    "            feed_path = f\"{feed_folder}/{ddmmyyyy}/feed.xml\"\n",
    "\n",
    "            # if file does not exist, skip\n",
    "            if not os.path.exists(feed_path):\n",
    "                progress.update(task, advance=1)\n",
    "                continue\n",
    "\n",
    "            # parse the feed\n",
    "            feed, articles = parse_feed(feed_path, url)\n",
    "\n",
    "            # insert the feed into the database\n",
    "            c.execute(\"INSERT OR IGNORE INTO feeds VALUES (?, ?, ?, ?, ?, ?, ?)\", feed)\n",
    "\n",
    "            # insert the articles into the database\n",
    "            c.executemany(\"INSERT OR IGNORE INTO articles VALUES (?, ?, ?, ?, ?, ?, ?)\", articles),\n",
    "\n",
    "            conn.commit() \n",
    "\n",
    "            progress.update(task, advance=1)\n",
    "    c.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the language of the feeds\n",
    "\n",
    "The next step is to update the primary language of a feed. This is done by checking what is the highest number of articles with a certain language.\n",
    "\n",
    "What the following SQLite query does, it to group by language and count the number of articles for each language. Then we order by the count in descending order and we limit the result to 1. This way, we get the language with the highest number of articles.\n",
    "\n",
    "We have to take that result and to update the `feeds` table with the new language.\n",
    "\n",
    "Note: it doesn't seems possible to do that in SQLite directly, if I am missing some feature of the query language, please propose a better solution and submit a PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_articles_lang_per_feeds():\n",
    "    \"\"\"Get the count of articles per language per feed\"\"\"\n",
    "    conn = connect_feeds_db()\n",
    "    c = conn.cursor()\n",
    "    c.execute('''SELECT\n",
    "                    fa.language,\n",
    "                    fa.id\n",
    "                 FROM (\n",
    "                    SELECT\n",
    "                        feeds.id,\n",
    "                        feeds.url,\n",
    "                        articles.lang AS language,\n",
    "                        COUNT(*) AS lang_count\n",
    "                    FROM feeds\n",
    "                    LEFT JOIN articles ON articles.feed_id = feeds.id\n",
    "                    GROUP BY feeds.id, feeds.url, articles.lang\n",
    "                    ORDER BY feeds.id, lang_count DESC\n",
    "                 ) AS fa\n",
    "                            \n",
    "                 GROUP BY fa.id''')\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the feeds table with the new languages\n",
    "\n",
    "The next step is to take those results and to update the `feeds` table with the new language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def update_feeds_with_languages(rows):\n",
    "    \"\"\"Update the feeds database with the language of the feed\"\"\"\n",
    "    conn = connect_feeds_db()\n",
    "    c = conn.cursor()\n",
    "    c.executemany(\"UPDATE feeds SET lang = ? WHERE id = ?\", rows)\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Small Web Index\n",
    "\n",
    "This utility function is used to remove all the feeds that have been tagged as non-english. For the moment, only the ones that have been tagged with a non-english language will be added, the ones that the current heuristic couldn't determine the core language will be left in the index. Further work will be required for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_non_english_feeds():\n",
    "    \"\"\"Return the list of non-english feeds URL\"\"\"\n",
    "    conn = connect_feeds_db()\n",
    "    c = conn.cursor()\n",
    "    c.execute('''SELECT url \n",
    "                 FROM feeds \n",
    "                 WHERE lang <> 'en' and lang <> '' \n",
    "                 ORDER BY lang DESC''')\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to remove the feeds URLs from the Small Web index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_cleaned_small_web_index():\n",
    "    \"\"\"Return the cleaned small web index\"\"\"\n",
    "\n",
    "    index = get_small_web_feeds()\n",
    "    non_english_feeds = get_non_english_feeds()\n",
    "\n",
    "    # remove non-english feeds from the index\n",
    "    for feed in non_english_feeds:\n",
    "        index.remove(feed[0])\n",
    "\n",
    "    # order the index by feed id\n",
    "    index.sort()\n",
    "\n",
    "    # write the index in a new text file\n",
    "    with open('smallweb.txt', 'w') as f:\n",
    "        for url in index:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "    return index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
